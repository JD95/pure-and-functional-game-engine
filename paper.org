# org-mode settings
#+STARTUP: indent
#+STARTUP: hidestar

# paper meta 
#+TITLE: A Pure and Parallel Game Engine
#+AUTHOR: Jeffrey Dwyer
#+DATE: 11/15/2017
#+OPTIONS: toc:nil

# latex options
#+LATEX_HEADER: \usepackage[margin=1.0in]{geometry}
#+LATEX_HEADER: \usepackage{apacite}
#+LATEX_HEADER: \usepackage{listings}
#+LATEX_HEADER: \usepackage{setspace}
#+LATEX_HEADER: \usepackage{appendix}
#+LATEX_HEADER: \linespread{2.0}
#+LATEX_CLASS_OPTIONS: [12pt]

\begin{abstract}

This paper addresses the limited capacity of modern game engines take advantage of multiple processors by applying techniques from purely functional languages to achieve a design which allows for parallelization by default. This design will then be structured using functional reactive programming and optimized using various techniques like compact regions and fusion.

\end{abstract}

* Introduction

Game engines stand in the cross roads of many fields, benefiting from research ranging from physical simulations to artificial intelligence. Popular engines like Unity break down into several modules namely artificial intelligence, physics, scripting, input, multimedia, and networking cite:messaoudi_dissecting_2015. 

The high performance demanded of games forces their engines to be optimized so that all of the intensive calculations for a single frame of the game can fit within a mere sixteen millisecond window. This restriction allows for a frame rate of sixty frames per second or the rate at which the human eye cannot notice the frame changes. The common approach for the past few decades has been to optimize engines for single processor machines. The famous Moore's Law predicted a doubling of computing power every two years so this tactic was reasonable. cite:present_cramming_2000. This pace was continuous until around 2015 when companies like Intel announced that their pace had slowed to almost two and a half years with a predicted geometric slowing every year. With processors reaching the physical limits of their individual capabilities, there has been a movement toward mutlicore design in software to compensate cite:theis_end_2017.

Unfortunately, years of optimization for a single core processor leaves many of the most popular game engines unable to adapt to multicore designs without massive reworking cite:anderson_case_2008. This is unfortunate because many aspects of modern engine architectures lend themselves toward parallel execution. If the industry can no longer rely on the improvements of single processors to continue pushing the boundaries of what is possible in video games, then engines must be redesigned to take advantage of new multi-core processors. 

This paper will first examine the structure of modern game engines and then identify which pieces of the program can take advantage of parallel execution. In order to avoid the common pitfalls of parallel and concurrent programming in traditional languages, the possibility of using functional languages in designing the engine will be explored. 

* Game Engine Architecture

All graphical programs follow a similar flow in their execution. First, they will collect input from user interfaces like keyboards and the mouse. This input information is then fed into an internal logic for the system which then calculates the a new system state cite:czaplicki_asynchronous_2013. The system state is finally converted into the displayed image on the computer screen and the process repeats. Game engines follow a similar pattern, except the internal logic is typically divided more explicitly. 

Tagliasacchi et. al describes the engine update as a flow of data from user input to the resulting image. The process begins with user input given to the individual entities within the virtual scene which then decide what actions they are going to take that frame. Once the entities have calculated how they will change the world and affect other entities, these changes are given to a physics simulation which then implements and movements or forces exerted within the world. Any collisions that occur, with the ground for example, are taken into account and the entities are allowed to react to this information. At this point, the state for the frame has been calculated and the positions and orientations of models within the world are passed on to a rendering process which will create a frame to be displayed cite:tagliasacchi_cascade:_2008.

Modern games have potentially thousands of complex entities within the game world, putting even more pressure on the engine to complete its tasks as soon as possible. Typical tactics to achieve speed involve special algorithms to minimize work, minimizing the number of allocations made in system memory, and keeping related information closer in memory to optimize spatial locality. These techniques apply to multicore designs as well as single processor architectures. There are however, certain advantages that a multicore system would have over standard implementations today.
 
* Achieving a Parallel Design 

For two tasks to be run at the same time, their results must be independent of each other. While data in a game engine flows through the system in a linear fashion, there are calculations which can be completed in parallel. For example, each unit of artificial intelligence within the system might be able to decide on its next action based solely on how the world was configured in the previous frame. Each unit simply needs access to the previous state of the world and it can then be run at any time and in any order in relation to the other units. Most systems within the engine have this property with only two portions being completely serial, the construction of the render tree and interaction resolution cite:tulip_multi-threaded_2006.

The render tree is a data structure which the rendering module uses to determine the order in which to draw elements to the screen. With certain visual effects, like transparency, the order of rendering matters. For instance, transparent objects must be rendered after other objects in the scene to achieve the transparency. Other effects might be rendered over multiple passes or require the pixels from the previous pass in order to render, like motion blurs. Because this process depends heavily on a serial ordering it is impossible to parallelize, even in fully featured and popular game engines like Unity, most of the work is done in the rendering modules of the engine cite:messaoudi_dissecting_2015.

Interaction resolution consists of resolving conflicting interactions between entities. This mostly occurs after the physics simulation where objects collide with each other. The discrete nature of physics simulations allows for objects to temporarily intersect and thus must be corrected before the rest of the scene state may be calculated. Similar to rendering order, the results of future resolutions depend on previous ones, so this process cannot be parallelized. Beyond these two systems most of the remaining processes can incorporate some level of parallel execution. 

** Applying Parallelism

Tulip et. al outline several considerations to be taken when parallelizing the engine. First, the number of threads should be minimized to the number of cores available. Second, the creation and destruction of threads should be avoided while processing data. Third, the synchronization between threads should be minimized. Finally, the workload should be balanced across threads cite:tulip_multi-threaded_2006.

Several features which can be parallelized are: interpolation of animations, the application of lighting and textures, sound source contributions, rendering frames between updates cite:tulip_multi-threaded_2006. These separate tasks begin to outline different tasks within the engine. In his white paper, Andrews suggests divide the work of between different managers and using them to generate and distribute tasks to various worker treads. These tasks are created via messages between the different managers. When the next frame is to be calculated each manager would determine what work that subsystem needs to do. In this model, the managers serve as the main division of work between the different CPU cores. Balance across the various cores is achieved by only dividing the managers if there are enough threads to do so cite:andrews_designing_2009.

Although this design presents a simple method of converting existing architectures into parallel ones, there remain several challenges to overcome. One of the main concerns in concurrent programming is the possibility of dead locks, a situation in which many separate processes are waiting on each other in a cycle for some resource. While work is separated between managers, there is no guarantee that each subsystem will not affect the data needed by other systems. In most programming languages, there is no real restriction on what processes can modify, in fact engines often take advantage of this to increase efficiency in some cases cite:tagliasacchi_cascade:_2008. When translating these modules into a parallel system, any data which would be used by different processes would have to be put under a lock, allowing only one thread to work with the data at any given time. If a thread needs to modify data which is currently locked, it can only wait until the lock is lifted. If the waiting queue for locks ever becomes cyclical, then the system stops because no work can be done, a dead lock. 

One technique to avoid dead locks is software transactional memory (STM). This methodology performs small, reversible tasks which either complete successfully or are rolled back. Atomicity, a name given to the previous features, is what allows for normal locking to be avoided all together using STM. Lock free data structures using STM are faster than their locking counterparts, however programming using STM is complex in languages that do not directly support it cite:discolo_lock_2006. These challenges are not unique to game engines and their solution may come from a more general approach to simplifying parallel design.

** Functional Programming

In his Turing award lecture, John Backus posed the question of whether programming languages could grow out of their trend of becoming larger, but not more expressive. He noted that the many changeable parts of existing languages have little to no expressive power, thus leading to many features being built into the language itself. These properties make such languages difficult to reason about. He concluded by proposing functional programming as an alternative cite:backus_can_1978. 

Functional programming is a model of computation based on the Lambda Calculus of Alonzo Church and naturally lends itself to parallel computing given the semantic differences from normal programming styles cite:backus_can_1978. Instead of building a system using a series of steps, programs can be thought of as a series of almost algebraic expressions.

\begin{spacing}{0.5}
\begin{lstlisting}[language=Haskell, caption=Example of lambda evaluation]

(\lambda x. \lambda y. add x y) 5 6
(\lambda y. add 5 y) 6
(add 5 6)
11 

\end{lstlisting}
\end{spacing}


Semantically, lambda calculus evaluates programs by replacing input symbols with their input. In traditional languages this is achieved by "passing" values as parameters into functions. Unlike such languages, the entire structure of a functional program is built within nested lambda expressions. The runtime of such a language differs drastically from languages like C, with the order of evaluation taken out of the hands of the programmer cite:jones_implementing_1993. In functional languages like Haskell, this leads to far simpler programs. For example, the classic quicksort algorithm's semantics are obscured in languages like C.

\begin{spacing}{0.5}
\begin{lstlisting}[language=C, caption=Quicksort in C]
void quicksort(int *A, int len) {
  if (len < 2) return;
 
  int pivot = A[len / 2];
 
  int i, j;
  for (i = 0, j = len - 1; ; i++, j--) {
    while (A[i] < pivot) i++;
    while (A[j] > pivot) j--;
 
    if (i >= j) break;
 
    int temp = A[i];
    A[i]     = A[j];
    A[j]     = temp;
  }

  quicksort(A, i);
  quicksort(A + i, len - i);
}
\end{lstlisting}
\end{spacing} 
\vspace{5mm}

However, in languages like Haskell, the semantics of the algorithm can be expressed directly in an almost mathematical definition. The left side of the equation is the name of the function and it's inputs with the resulting expression on the right, much like a function in algebra. This maps to the parenthesized form of lambdas shown before.

\begin{spacing}{0.5}
\begin{lstlisting}[language=Haskell, caption=Quicksort in Haskell] 
qsort :: Ord a => [a] -> [a]
qsort [] = []
qsort (x:xs) = qsort ys ++ x : qsort zs
  where
    (ys, zs) = partition (< x) xs 

\end{lstlisting}
\end{spacing}
\vspace{5mm}

The differences are not only visual, the semantics of the language itself are simpler. One of the main challenges with programming is keeping track of how the state application changes throughout execution. These changes could be updates to values like a name or whether a button is turned on or off. Typical languages like C allow the user to change program state anywhere at anytime, which causes problems when trying to run tasks in parallel. 

#+CAPTION: The Quicksort function as a graph
[[./function-graph-example.png]]

Functional languages can represent each computation as a graph. Because there are no dependencies on state between any of the leaves in such graphs, each can be evaluated and reduced in any order or at the same time. It is this property which makes languages like Haskell so easy to run in parallel. Most programs need only a small annotation to indicate that the expression should be evaluated at the same time as the others.

\begin{spacing}{0.5}
\begin{lstlisting}[language=Haskell, caption=Parallel Quicksort in Haskell] 
qsort :: Ord a => [a] -> [a]
qsort [] = []
qsort (x:xs) = (qsort ys `using` reseq) ++ x : (qsort zs `using` rseq)
  where
    ys = filter (< x) xs `using` rseq
    zs = filter (>= x) xs `using` rseq

\end{lstlisting}
\end{spacing}
\vspace{5mm}

While this solves one problem, it causes another in that creating new values for every calculation comes at a cost which is unacceptable in programs like game engines. It takes more time to create new data than it does to update existing data. Another issue is that functional languages, Haskell included, manage their memory using automatic processes called garbage collection. Every so often, the program will be paused so memory which is no longer needed can be freed. Again, these types of delays prevent the kind of performance needed by modern video games. 

The rest of this paper will address redesigning the structure of a typical game engine to fit a functional style and then cover various techniques for reducing or eliminating the performance concerns of this style.
 
* Representing the game loop using functional reactive programming 

Game engines are similar to most other graphical programs that users would interact with cite:tulip_multi-threaded_2006. Graphical application frameworks today take user input and allow for individual components of the application handle the inputs. An example would be a button which, when clicked, modifies the state of a pop up to be activated. This localized view of the system has several draw backs, but most important to the design of the engine, it obscures which entities have interactions with other entities. 

In a functional paradigm, where arbitrary modifications of data are not allowed, the relations between different entities must be made explicit. As such, there is a movement toward so called "Reactive" systems which model the application like a circuit in which information flows through to produce an output, the displayed user interface. The functional approach to these semantics is aptly called, "Functional Reactive Programming" or FRP. In his formulation of FRP, Elliot defined a system based on two primitive types, Behaviors, Events, and a set of combinators for generating new values based on those primitives cite:elliott_push-pull_2009. 

Elliot describes behaviors as functions from time to a value. An example of this would be a ball in the air whose height is dependent on time and the velocity of the ball. As time progresses, the height of the ball decreases. These behaviors can be used to create more behaviors which depend on other streams of values. And example of a more complex behavior would be an object in the virtual world which was dependent on the location of the player's mouse at any given time. Character animations also fall under the category of a behavior. As time progresses, the animation progresses one frame, changing the configuration of the model skeleton causing movement.

An Event is a function from time to a possible value. The classic example of an event would be a mouse click. If one where to plot the function of an event it would remain mostly at zero until the event occurred, which would be visible as a small spike in the value before it returns to nothing. Events can be used to model discrete occurrences within the system which are then used by behaviors to alter the interface cite:wan_functional_2000.

#+CAPTION: A reactive network for unit position
[[./frp-unit-example.png]]

In the above example, we can see two behaviors, the player unit's position and the alarm's position. Over time, the player's position will change which causes the active state of the alarm to be recalculated. Here, the difference between the player's position and the alarm is calculated and then that result is checked to see if it is less than five. The alarm triggering can be considered an event since it only occurs at discrete points in time. This model of entity behavior is similar to that of the Unreal engine's blueprint system.

#+CAPTION: Blueprints in the Unreal Engine
[[./unreal-blueprints.jpg]]
 
** Alternative and Improved formulations of FRP

Although FRP creates a rich and expressive style to model a game engine with, there are several performance issues with the semantics as originally defined. For instance, given that all values are dependent on time, all values within the system must be constantly recalculated, which causes large amounts of wasteful work calculating values which have not changed. Another problem is that the original semantics also force all previous values for behaviors and events to be stored for the duration of the program. As time progresses, the memory usage slowly builds. This is unacceptable for a game engine, but there are other formulations of FRP which drastically reduce the problems with Elliot's initial design.

In Czaplicki and Chong's formulation of FRP, changes do not propagate unless a discrete event occurs. This change, while unfaithful to the original semantics of FRP, is much more suited for graphical interfaces due to that face that the user can only interact with the system in discrete ways cite:czaplicki_asynchronous_2013. While certain systems would change continuously with time, like the physics simulation, the vast majority of components will only ever change with discrete events. This reduces the amount of recalculations needed.

In his reformulation of the original FRP semantics, Elliott introduced the idea of reactive values and push-pull semantics to address the same performance issues. Reactive values allow for changes to certain values to be propagated or pushed through the system, leaving pull based updates to time dependent events. These reactive values allow for the same mental model for behaviors to be used without the performance loss cite:elliott_push-pull_2009.

In Nilsson et. al a continuation formulation of FRP is presented where behaviors can be modeled as transition functions which return how they will behave after a discrete time change. By removing behaviors as first class within the framework, the previous performance costs are reduced and the semantics of working with behaviors is simplified cite:nilsson_functional_2002.

The game loop for a parallel engine can thus be described as a flow which takes user input, time, and the state of the world in the previous frame and then uses each as events and behaviors for calculating the next frame. A stream of messages can be used to model the communication of certain entities with each other. These messages can be interpreted at a synchronization point before rendering which interprets the messages and uses STM to safely modify data or otherwise pass the message on to the next frame to be handled by the appropriate entities.

In this design, all entities can calculate their changes to themselves and the world in parallel and thus the workload can be balanced across the processors at a fine grained level. This is an improvement over the manager approach to parallelization in that managers were forced onto a single processor and unable to share their workloads with other managers which might have finished their tasks sooner.
  
* Addressing Efficency Concerns 

Although pure functional programming allows for expressivity and simple parallelization, there are performance costs which must be addressed. 

Pure code also causes new allocations for every change or update to existing data. Operations on existing data are cheap time-wise, but creating new data is expensive. Many optimizations done by modern engines focus on reusing existing data to prevent new allocations whenever possible. Luckily, pure functional languages allow for the compiler to perform many complex optimizations not possible in other languages. Once technique for reducing allocations is to eliminating intermediate data from being created between data producers and data consumers, also known as fusion. 

** Fusion

Fusion eliminates intermediate data structures like a lists due to the properties of the functions. In the functional style, functions like map, fold, and filter are common tools used to manipulate data structures. Due to referential transparency, functions can be manipulated almost like algebraic expression in math where redundancies can be removed. 

\begin{spacing}{0.5}
\begin{lstlisting}[language=Haskell]

map (g . f) = map g . map f

\end{lstlisting}
\end{spacing}
\vspace{5mm}

In the case of the function map, which applies a function to all values of a list, the composition of two maps is equal to mapping the composition of both functions. So rather than create a new list for the result of map f and then a new structure for map g, the optimized version simply applies (g . f) to every element of the input list and creates only a single new list. List are not the only structures which can benefit from fusion. 

Meijer et. al formalized several recursion schemes which could replace normal recursion as the basic building block of functional programs. These recursion schemes were divided into two categories, anamorphisms, which produce new values and catamorphisms, which consume values. In general, it is the pairing of an anamorpic producer and a catamorphic consumer that allows for fusion to occur cite:meijer_functional_1991. Fusion can occur for any recursive data structures, not just lists cite:bernardy_composable_2016.

Certain modules within the engine serve only to produce or consume data. Several producers are user input and networking. Several consumers are sound and rendering cite:tulip_multi-threaded_2006. By modeling these systems with fusion in mind, we can eliminate some unnecessary allocations.

Another technique used to reduce allocations is by sharing the results of previous computations. This however causes a problem as the question of when a certain value will no longer be needed by the rest of the program is a difficult question to answer by simply analyzing the code. The modern solution to this problem is an automatic memory management process called garbage collection. This process freezes the program execution and scans memory for data which is no longer being used so it can be freed. Without garbage collection, functional languages as they are implemented today, would quickly run out of memory. Beyond periodically stopping the program altogether, garbage collection removes the control of memory layout from the programmer and prevents the kinds of optimizations needed for maximum performance in a game engine. In order for a functional language to be used to build an engine, garbage collection must be reduced or eliminated.

** Minimizing Garbage Collection

Haskell uses a parallel generational garbage collection which Marlow et. al note favors short lived data cite:marlow_parallel_2008. The generational garbage collector organizes memory such that younger objects are created in one location and gradually "age". When an older generation is collected so to are the generations younger than it. One added benefit of immutability is that it allows for efficient checking of garbage given that "old" data cannot reference new data. This means that when a younger generation is collected, the garbage collector can stop its swap when it reaches data in an older generation cite:marlow_parallel_2008.

Further optimizations can be made by making use of a technique called compact regions. Yang et. al demonstrated that if an immutable structure has no references to data beyond its own, then the structure can be compressed into a contiguous region in memory cite:yang_efficient_2015. This optimization is vital to long lived data like the many character models, sound files, images, and terrain data that need to survive the length of a game. With this memory loaded into a compact region, the garbage collector would only to need to check for a single reference to the region instead of having to swap the entire structure. 

More over, Yang et. al discovered that compact regions can be written directly to files or sent over the network with the internal pointers need simply be offset to match their new spot in memory  cite:yang_efficient_2015. This would be ideal for a game involving networking. Serialization is a expensive even in traditional programming languages.

** Eliminating Garbage collection

Languages like C++ and Rust have mechanisms which track the lifetime of values throughout a program and free memory when "owner" values are freed. This system is possible to emulate in functional languages through an alteration of the type system to include linear types. A linear types force all values to be used and used only once cite:wadler_linear_1990. If values are not shared between computations, then the compiler can optimize in ways it could not otherwise.

\begin{spacing}{0.5}
\begin{lstlisting}[language=Haskell]

f x = (x,x) -- Error! Cannot duplicate value! 
f x y = x   -- Error! Value 'x' not used!

\end{lstlisting}
\end{spacing}
\vspace{5mm}

Bernardy et. al found that linear type systems could be added to existing, lazy languages like Haskell without modification to existing functions. This addition allows for O(1) updates to the value instead of an O(n) copy cite:bernardy_linear_2017. Linear values would reduce the amount of memory used by the program and thus reduce garbage collection. Lafont used linear types to develop a language which used a mixture of strict and lazy evaluation without garbage collection cite:lafont_linear_1988. By taking advantage of linear types within the game engine, many values can be managed outside of the normal garbage collected memory space.


* Conclusions

By making use of modern research into functional programming languages, it appears possible to achieve a parallel game engine while maintaining an expressive system for designing games. Immutability and referential transparency make any process within the engine trivially parallelizable. The traditional game loop translates into a functional reactive framework which allows various updates within the world to be modeled in a consistent way. Using software transactional memory, updates to the game state can be made without the dangers of dead locks. Using techniques like fusion, compact regions, and linear types, garbage collection can be minimized or even eliminated. While there may exist frameworks which allows for a game engine to be parallelized within traditional paradigms, the functional approach provides the most direct means to achieve parallel execution. 


bibliography:refs.bib
bibliographystyle:apacite

\begin{appendices}

* Experimental Plan

This paper outlines a possible design for a game engine designed for parallel execution. In order to measure the effectiveness of the design, a prototype implementation would be needed. The prototype would be subject to several benchmarking and performance profiling techniques.

** Data to be collected

There are several dimensions of game engine performance. CPU usage is measured in units of time a process spends working on the processing unit. This measurement does not include kernel interrupts. Given the nature of a parallel engine, the CPU metric would be extended to total utilization over multiple cores. RAM usage is measured in storage units of megabytes. Frames per second is a measure of how quickly the engine can render the next frame. Each of these dimensions will be measured over a period of time. 

** Equipment needed

These test will be run on several machines with different CPUs. Several multi-core architectures will be tested, ranging from dual core to eight core chip sets from both Intel and AMD.

** Research Methods

There will be three tests:
- Simulation of 40,000 particles.
- Simulation of 1,000 animated entities with 50,000 polygons each.

Particle simulations help test the performance of an engine with a shear number of simple entities. 40,000 particles is the current industry standard for these kinds of tests. The animated entity test measures how well the engine can handle high amounts of vertex information within a scene. The high memory load will test how much the system needs to garbage collect.

The performance results of the prototype engine will be compared to the performance of similar tests run in both the Unreal and Unity engines. Each test will run its simulation for roughly a minute and will be repeated several hundred times to allow for statistical analysis. Frames per second will be outputted by the engine itself, RAM usage will be monitored using system tools on the test systems, and CPU usage will be measured using the program Threadscope.

The results will be averaged for each machine and the results will be analyzed to see how the performance changes as the number of cores increases and compared between the two chip architectures.

\end{appendices}
